<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>FLLC-ICPR2020</title>
    <link>https://FLLC-ICPR2020.github.io/</link>
    <description>Recent content on FLLC-ICPR2020</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Mar 2020 21:01:41 +0800</lastBuildDate>
    
	<atom:link href="https://FLLC-ICPR2020.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Home</title>
      <link>https://FLLC-ICPR2020.github.io/post/home/</link>
      <pubDate>Fri, 13 Mar 2020 00:01:07 +0800</pubDate>
      
      <guid>https://FLLC-ICPR2020.github.io/post/home/</guid>
      <description>Latest News  The 2nd grand challenge of 106-point facial landmark localization will be held in conjunction with the International Conference on Pattern Recognition (ICPR) 2020, Milan, Italy.  Introduction Facial landmark localization is a very crucial step in numerous face related applications, such as face recognition, facial pose estimation, face image synthesis, etc. We hold the 2nd lightweight 106-point facial landmark localization competition in conjunction with ICPR 2020. The purpose is to make effort towards benchmarking lightweight facial landmark localization, which enable efficient system deployment.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://FLLC-ICPR2020.github.io/home/</link>
      <pubDate>Wed, 11 Mar 2020 21:01:41 +0800</pubDate>
      
      <guid>https://FLLC-ICPR2020.github.io/home/</guid>
      <description>Latest News  The 2nd grand challenge of 106-point facial landmark localization will be held in conjunction with the International Conference on Pattern Recognition (ICPR) 2020, Milan, Italy.  Introduction Facial landmark localization is a very crucial step in numerous face related applications, such as face recognition, facial pose estimation, face image synthesis, etc. We hold the 2nd lightweight 106-point facial landmark localization competition in conjunction with ICPR 2020. The purpose is to make effort towards benchmarking lightweight facial landmark localization, which enable efficient system deployment.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://FLLC-ICPR2020.github.io/dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://FLLC-ICPR2020.github.io/dataset/</guid>
      <description>Training data: The dataset is an incremental of JD-landmark[1], consisting of *** images with 106-point landmark annotation. The images and landmarks will be released on 2020/3/30.
Download Link:
Validation data: It consists of 2000 images, covering large variations of pose, expression and occlusion, are selected from open source web face dataset [2]. During the validation phase, the participants could send the predicted results of the validation images to (email). We will return the results to the participants and update the leaderboard.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://FLLC-ICPR2020.github.io/evaluation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://FLLC-ICPR2020.github.io/evaluation/</guid>
      <description>Overview The average Euclidean point-to-point error normalized by the bounding box size is</description>
    </item>
    
    <item>
      <title></title>
      <link>https://FLLC-ICPR2020.github.io/leaderboard/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://FLLC-ICPR2020.github.io/leaderboard/</guid>
      <description>Overview We offer a benchmark suite together with an evaluation server, such that authors can upload their results and get a ranking. We offer a Dataset that contains more than 50000 pictures, including 30462 images for training set, 10000 images for validation set and 10000 images for test set. If you would like to submit your results, please register, login, and follow the instructions on our submission page.
Note: We only display the highest submission of each person.</description>
    </item>
    
  </channel>
</rss>